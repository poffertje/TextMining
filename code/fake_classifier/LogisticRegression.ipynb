{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression fake classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poffertje/TextMining/blob/master/code/fake_classifier/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### This code is strongly based of Amazon review classifier (https://t-lanigan.github.io/amazon-review-classifier/)"
      ],
      "metadata": {
        "id": "3GEQXnZletRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvuWICD1Budo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3bacab-efb9-4b82-b864-ededa97cc531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import string\n",
        "import joblib\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as scipy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from six.moves import cPickle as pickle\n",
        "from time import time\n",
        "\n",
        "pd.options.display.max_rows = 15\n",
        "np.set_printoptions(precision = 4, suppress=True)\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "4-v2vHHZF5Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "WNrd0FsOGHku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset import"
      ],
      "metadata": {
        "id": "pqXJpbku4THE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set variable containing dataset path\n",
        "FakeReviews = \"/content/gdrive/Shareddrives/Minecraft/Datasets/final_classifier_dataset.csv\""
      ],
      "metadata": {
        "id": "Qzpm0iFjYw3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import dataset\n",
        "dataset = pd.read_csv(FakeReviews,index_col=0)"
      ],
      "metadata": {
        "id": "p0F3YefyYuhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show dataset\n",
        "display(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QHylx26E352F",
        "outputId": "3a49b62c-380d-4a2d-e251-31da12f82a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        userID  productID  rating  label        date  \\\n",
              "0         5045          0     1.0      0  2014-09-08   \n",
              "1         5046          0     3.0      0  2013-10-06   \n",
              "2         5047          0     5.0      0  2014-11-30   \n",
              "3         5048          0     5.0      0  2014-08-28   \n",
              "4         5049          0     5.0      0  2013-07-16   \n",
              "...        ...        ...     ...    ...         ...   \n",
              "159995    6805        839     4.0      1  2014-01-14   \n",
              "159996   22381        839     5.0      1  2014-01-13   \n",
              "159997   10265        839     1.0      1  2014-01-09   \n",
              "159998   69957        839     2.0      1  2014-01-09   \n",
              "159999   69958        839     3.0      1  2014-01-06   \n",
              "\n",
              "                                                   review  review_length  \\\n",
              "0       This was the worst experience I've ever had a ...            248   \n",
              "1       This is located on the site of the old Spruce ...             50   \n",
              "2       I enjoyed coffee and breakfast twice at Toast ...            233   \n",
              "3       I love Toast! The food choices are fantastic -...            152   \n",
              "4       The egg on an English muffin (their take on eg...             73   \n",
              "...                                                   ...            ...   \n",
              "159995  the menu is small, but the dishes are well pre...             71   \n",
              "159996  You have to get the nachos. They are our of th...             52   \n",
              "159997  I revoke my previous praise for happy hour at ...             81   \n",
              "159998  I had pretty high expectations for El Rey, but...             92   \n",
              "159999  Eh, this is pretty mediocre Mexican food serve...             31   \n",
              "\n",
              "        average_product_rating  average_user_rating  extreme_count_ratio  \\\n",
              "0                     3.643678             1.000000             1.000000   \n",
              "1                     3.643678             3.250000             0.000000   \n",
              "2                     3.643678             5.000000             0.000000   \n",
              "3                     3.643678             5.000000             0.000000   \n",
              "4                     3.643678             5.000000             0.000000   \n",
              "...                        ...                  ...                  ...   \n",
              "159995                3.507634             2.666667             0.500000   \n",
              "159996                3.507634             4.583333             0.000000   \n",
              "159997                3.507634             3.862069             0.017241   \n",
              "159998                3.507634             2.500000             0.000000   \n",
              "159999                3.507634             4.000000             0.000000   \n",
              "\n",
              "        exclaim_cnt  all_cap  sub_cnt  \n",
              "0                 0        4        9  \n",
              "1                 0        1        0  \n",
              "2                 2        9       11  \n",
              "3                 2        3        3  \n",
              "4                 0        0        0  \n",
              "...             ...      ...      ...  \n",
              "159995            1        0        1  \n",
              "159996            0        2        4  \n",
              "159997            1        5        7  \n",
              "159998            0        5        7  \n",
              "159999            0        0        0  \n",
              "\n",
              "[160000 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-854e6d20-d07e-4514-b5f3-31fedd2018cf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>productID</th>\n",
              "      <th>rating</th>\n",
              "      <th>label</th>\n",
              "      <th>date</th>\n",
              "      <th>review</th>\n",
              "      <th>review_length</th>\n",
              "      <th>average_product_rating</th>\n",
              "      <th>average_user_rating</th>\n",
              "      <th>extreme_count_ratio</th>\n",
              "      <th>exclaim_cnt</th>\n",
              "      <th>all_cap</th>\n",
              "      <th>sub_cnt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5045</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-09-08</td>\n",
              "      <td>This was the worst experience I've ever had a ...</td>\n",
              "      <td>248</td>\n",
              "      <td>3.643678</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5046</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2013-10-06</td>\n",
              "      <td>This is located on the site of the old Spruce ...</td>\n",
              "      <td>50</td>\n",
              "      <td>3.643678</td>\n",
              "      <td>3.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5047</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-11-30</td>\n",
              "      <td>I enjoyed coffee and breakfast twice at Toast ...</td>\n",
              "      <td>233</td>\n",
              "      <td>3.643678</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5048</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-08-28</td>\n",
              "      <td>I love Toast! The food choices are fantastic -...</td>\n",
              "      <td>152</td>\n",
              "      <td>3.643678</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5049</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2013-07-16</td>\n",
              "      <td>The egg on an English muffin (their take on eg...</td>\n",
              "      <td>73</td>\n",
              "      <td>3.643678</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159995</th>\n",
              "      <td>6805</td>\n",
              "      <td>839</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-01-14</td>\n",
              "      <td>the menu is small, but the dishes are well pre...</td>\n",
              "      <td>71</td>\n",
              "      <td>3.507634</td>\n",
              "      <td>2.666667</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159996</th>\n",
              "      <td>22381</td>\n",
              "      <td>839</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-01-13</td>\n",
              "      <td>You have to get the nachos. They are our of th...</td>\n",
              "      <td>52</td>\n",
              "      <td>3.507634</td>\n",
              "      <td>4.583333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159997</th>\n",
              "      <td>10265</td>\n",
              "      <td>839</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-01-09</td>\n",
              "      <td>I revoke my previous praise for happy hour at ...</td>\n",
              "      <td>81</td>\n",
              "      <td>3.507634</td>\n",
              "      <td>3.862069</td>\n",
              "      <td>0.017241</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159998</th>\n",
              "      <td>69957</td>\n",
              "      <td>839</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-01-09</td>\n",
              "      <td>I had pretty high expectations for El Rey, but...</td>\n",
              "      <td>92</td>\n",
              "      <td>3.507634</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159999</th>\n",
              "      <td>69958</td>\n",
              "      <td>839</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-01-06</td>\n",
              "      <td>Eh, this is pretty mediocre Mexican food serve...</td>\n",
              "      <td>31</td>\n",
              "      <td>3.507634</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>160000 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-854e6d20-d07e-4514-b5f3-31fedd2018cf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-854e6d20-d07e-4514-b5f3-31fedd2018cf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-854e6d20-d07e-4514-b5f3-31fedd2018cf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset splitting"
      ],
      "metadata": {
        "id": "mUoTtNwRwS5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into \n",
        "x_train, x_test, y_train, y_test = train_test_split(dataset, dataset['label'],test_size=0.2, random_state=25)"
      ],
      "metadata": {
        "id": "oDEpW1jqzUP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check train set label distribution\n",
        "x_train.value_counts(\"label\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-kt7KyL9e8I",
        "outputId": "4ac2dd07-9fb9-4897-fa77-25aef4adaaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    64047\n",
              "1    63953\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating correlation matrix"
      ],
      "metadata": {
        "id": "937-mlgMwYWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate correlation table from training data\n",
        "correlation_table = x_train.corr().round(decimals=2)"
      ],
      "metadata": {
        "id": "jW5tLZ71sA-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export correlation table\n",
        "correlation_table.to_csv(\"/content/gdrive/Shareddrives/Minecraft/Our_Models/LogisticRegression/correlation_table.csv\")"
      ],
      "metadata": {
        "id": "NW75ymG4J4u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define our own tokenizing function that we will pass into the TFIDFVectorizer. We will also stem the words here.\n",
        "def tokens(x):\n",
        "    x = x.split()\n",
        "    stems = []\n",
        "    [stems.append(stemmer.stem(word)) for word in x]\n",
        "    return stems"
      ],
      "metadata": {
        "id": "MI9EaiHDycqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating feature space for LR"
      ],
      "metadata": {
        "id": "buMqH9Oy4f47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a stemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "os.chdir('/content/gdrive/Shareddrives/Minecraft/Our_Models/LogisticRegression/')\n",
        "#loads pickle if exists, extracts and pickles if it doesn't\n",
        "if os.path.exists('features.pickle') and os.path.exists('vectorizer.pickle'):\n",
        "    print ('Pickled file already present, loading...')\n",
        "    features = pickle.load( open( \"features.pickle\", \"rb\" ) )\n",
        "    vectorizer = pickle.load( open( \"vectorizer.pickle\", \"rb\") )\n",
        "    print ('Pickle file loaded.')\n",
        "else:\n",
        "    #define the vectorizer\n",
        "    vectorizer = TfidfVectorizer(tokenizer = tokens, stop_words = 'english', ngram_range=(1, 3), min_df = 0.01)\n",
        "    #fit the vectorizers to the data.\n",
        "    x_train.loc[:, 'review'] = x_train['review'].str.lower()\n",
        "    x_train['review']=x_train['review'].apply( lambda x: remove_punctuation(x))\n",
        "    features = vectorizer.fit_transform(x_train['review'].values.astype(str))\n",
        "    length = np.array(list(x_train.review_length)).reshape(features.shape[0], 1)\n",
        "    xtreme_ratio = np.array(list(x_train.extreme_count_ratio)).reshape(features.shape[0],1)\n",
        "    features = scipy.sparse.hstack((features,scipy.sparse.csr_matrix(length)))\n",
        "    features = scipy.sparse.hstack((features,scipy.sparse.csr_matrix(xtreme_ratio)))\n",
        "    features = scipy.sparse.csr_matrix(features)\n",
        "    pickle.dump(features, open(\"features.pickle\", \"wb\"))\n",
        "    pickle.dump(vectorizer, open(\"vectorizer.pickle\", \"wb\"))\n",
        "\n",
        "features"
      ],
      "metadata": {
        "id": "MQNJc1T0ydN7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e57437-3f8e-4722-b3bc-7f7e841c9b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pickled file already present, loading...\n",
            "Pickle file loaded.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<128000x856 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 4158003 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining helper methods\n",
        "\n"
      ],
      "metadata": {
        "id": "AxXGzKuswx_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function for removing punctuation\n",
        "def remove_punctuation(text):\n",
        "  return text.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "mrwgXyxnIN_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count number of exclamation marks\n",
        "def count_exlaim(review):\n",
        "    count = 0\n",
        "    for i in range(len(review)):\n",
        "        if review[i] == '!':\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "# count number of capital words\n",
        "def count_caps(review):\n",
        "    count = 0\n",
        "    for item in review.split():\n",
        "        if item.isupper():\n",
        "            count += 1\n",
        "    return count"
      ],
      "metadata": {
        "id": "0G9i5OBwIJOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# additional columns to be added to input data\n",
        "def adding_columns(data):\n",
        "  data[\"review_length\"] = data['review'].str.split().str.len()\n",
        "  data[\"average_product_rating\"] = data.groupby('productID')['rating'].transform('mean')\n",
        "  data[\"average_user_rating\"] = data.groupby('userID')['rating'].transform('mean')\n",
        "  nr_rows = data.groupby('userID').size().astype(float).reset_index(name=\"nr of rows\")\n",
        "  extreme_count = (data.groupby('userID')['rating'].apply(lambda x: (x == (1.0 or 5.0) ).sum())).reset_index(name=\"extreme_count_ratio\")\n",
        "  extreme_count[\"extreme_count_ratio\"] = extreme_count[\"extreme_count_ratio\"].astype(float).div(nr_rows[\"nr of rows\"].values,axis=0)\n",
        "  data = pd.merge(data, extreme_count, how='left', on = 'userID')\n",
        "  data[\"nr_of_reviews\"] = data.groupby('userID')[\"userID\"].transform('count')\n",
        "  return data"
      ],
      "metadata": {
        "id": "KaabVfUzk3wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert input data to features (input for the model)\n",
        "def features_conversion(data):\n",
        "  os.chdir('/content/gdrive/Shareddrives/Minecraft/Our_Models/LogisticRegression/')\n",
        "  # drop NaN rows\n",
        "  data = data.dropna()\n",
        "  data.loc[:, 'review'] = data['review'].str.lower()\n",
        "  data['review']=data['review'].apply( lambda x: remove_punctuation(x))\n",
        "  vectorizer = joblib.load(\"vectorizer.pickle\")\n",
        "  features_created = vectorizer.transform(data['review'])\n",
        "  length = np.array(list(data.review_length)).reshape(features_created.shape[0], 1)\n",
        "  features_created = scipy.sparse.hstack((features_created,scipy.sparse.csr_matrix(length)))\n",
        "  xtreme_ratio = np.array(list(data.extreme_count_ratio)).reshape(features_created.shape[0],1)\n",
        "  features_created = scipy.sparse.hstack((features_created,scipy.sparse.csr_matrix(xtreme_ratio)))\n",
        "  features_created = scipy.sparse.csr_matrix(features_created)\n",
        "  return features_created"
      ],
      "metadata": {
        "id": "cxwE70gfdETc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert data to features (compatible with feature space)"
      ],
      "metadata": {
        "id": "AFFisa7Kw378"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert training data to features\n",
        "x_train = features_conversion(x_train)"
      ],
      "metadata": {
        "id": "fSwIf6BC2BIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert testing data to features\n",
        "x_test = features_conversion(x_test)"
      ],
      "metadata": {
        "id": "d4VHCv3C2IyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training classifier and export model"
      ],
      "metadata": {
        "id": "VIye9ZDUw8za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to train (and store) classifier model\n",
        "def train_classifier(clf, X_train, y_train, store=False,name=None):\n",
        "    ''' Fits a classifier to the training data. '''\n",
        "    \n",
        "    # Start the clock, train the classifier, then stop the clock\n",
        "    start = time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    end = time()\n",
        "    \n",
        "    if store == True:\n",
        "      filename = name\n",
        "      joblib.dump(clf,filename)\n",
        "    # Print the results\n",
        "    print (\"Trained model in {:.4f} seconds\".format(end - start))"
      ],
      "metadata": {
        "id": "jHMdBDEj7lMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train classifier and store model\n",
        "os.chdir('/content/gdrive/Shareddrives/Minecraft/Our_Models/LogisticRegression/')\n",
        "train_classifier(LogisticRegression(), x_train,y_train,True,\"finalized_model.sav\")"
      ],
      "metadata": {
        "id": "YyCMTOjr6yX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6d2baf-36f4-4fd7-d088-88d77c041081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained model in 3.0388 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load in model\n",
        "os.chdir('/content/gdrive/Shareddrives/Minecraft/Our_Models/LogisticRegression/')\n",
        "final_model = joblib.load(\"finalized_model.sav\")"
      ],
      "metadata": {
        "id": "moMMSweqZMxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification report on test set"
      ],
      "metadata": {
        "id": "K_9xWpR_6XQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate and show classification report\n",
        "print(classification_report(y_test, final_model.predict(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUxmruC74l5k",
        "outputId": "21490451-62ca-4a8d-fe9a-b27d43f7bee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.71      0.69     15953\n",
            "           1       0.69      0.66      0.68     16047\n",
            "\n",
            "    accuracy                           0.68     32000\n",
            "   macro avg       0.68      0.68      0.68     32000\n",
            "weighted avg       0.68      0.68      0.68     32000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing production test set"
      ],
      "metadata": {
        "id": "Wmk8QdlKxIKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set variable to path containing production test set\n",
        "test_1814 = \"/content/gdrive/Shareddrives/Minecraft/Datasets/8April_sample_production_set.csv\""
      ],
      "metadata": {
        "id": "qDSBUi36ndN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the file\n",
        "test_1814 = pd.read_csv(test_1814)"
      ],
      "metadata": {
        "id": "rj95xcBsns4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding the labels from -1 and 1 to 0 and 1\n",
        "encode_label = {-1 : 0, 1 : 1}"
      ],
      "metadata": {
        "id": "zwQFCDcVSevT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding \n",
        "test_1814['label'] = test_1814['label'].map(encode_label)"
      ],
      "metadata": {
        "id": "yt7XzKiYSkWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check distribution of target label in production test set\n",
        "test_1814.value_counts(\"label\")"
      ],
      "metadata": {
        "id": "Gfo9TmS0D5DB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6ba1f6-e6e6-4972-f27d-0b6498de3a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "1    167\n",
              "0     33\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add additional columns to be intepretable for the model\n",
        "test_1814 = adding_columns(test_1814)"
      ],
      "metadata": {
        "id": "fgGUxvu8lDJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dataframe to features\n",
        "test_1814_features = features_conversion(test_1814)"
      ],
      "metadata": {
        "id": "apWt_TGznwWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification report on production test set"
      ],
      "metadata": {
        "id": "5aBelMTbxTRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate and show classification report\n",
        "print(classification_report(test_1814[\"label\"], final_model.predict(test_1814_features)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9S3Sl0ilZ20",
        "outputId": "65ac4c9a-04cd-4b57-d6ec-526772691779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.64      0.39        33\n",
            "           1       0.90      0.68      0.78       167\n",
            "\n",
            "    accuracy                           0.68       200\n",
            "   macro avg       0.59      0.66      0.59       200\n",
            "weighted avg       0.80      0.68      0.71       200\n",
            "\n"
          ]
        }
      ]
    }
  ]
}